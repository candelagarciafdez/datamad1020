{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some web scraping exercises to practice your scraping skills using `requests` and `Beautiful Soup`.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the [response status code](https://http.cat/) for each request to ensure you have obtained the intended content.\n",
    "- Look at the HTML code in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract.\n",
    "- Check out the css selectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Resources\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all, gathering our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Trending', 'developers', 'on', 'GitHub', 'today', '·', 'GitHub', 'Skip', 'to', 'content', 'Sign', 'up', 'Why', 'GitHub?', 'Features', '→', 'Code', 'review', 'Project', 'management', 'Integrations', 'Actions', 'Packages', 'Security', 'Team', 'management', 'Hosting', 'Mobile', 'Customer', 'stories', '→', 'Security', '→', 'Team', 'Enterprise', 'Explore', 'Explore', 'GitHub', '→', 'Learn', '&', 'contribute', 'Topics', 'Collections', 'Trending', 'Learning', 'Lab', 'Open', 'source', 'guides', 'Connect', 'with', 'others', 'Events', 'Community', 'forum', 'GitHub', 'Education', 'GitHub', 'Stars', 'program', 'Marketplace', 'Pricing', 'Plans', '→', 'Compare', 'plans', 'Contact', 'Sales', 'Nonprofit', '→', 'Education', '→', 'Search', 'All', 'GitHub', '↵', 'Jump', 'to', '↵', 'No', 'suggested', 'jump', 'to', 'results', 'Search', 'All', 'GitHub', '↵', 'Jump', 'to', '↵', 'Search', 'All', 'GitHub', '↵', 'Jump', 'to', '↵', 'Sign']\n"
     ]
    }
   ],
   "source": [
    "pagina = requests.get(url)\n",
    "soup = BeautifulSoup(pagina.content, 'html.parser')\n",
    "print(soup.text.split()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below (with different names):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Laurent\nDanny Ryan\nArda Tanrıkulu\nDotan Simha\nFelix Angelov\nAlisdair McDiarmid\nGleb Bahmutov\nAnton Babenko\nFlorian Roth\nTanner Linsley\nNicolás Bevacqua\nFrost Ming\nFelix Lange\nKevin Papst\nTobias Klauser\nMathias Bynens\nFons van der Plas\nMiek Gieben\nRomain Rigaux\nBrad Fitzpatrick\nMichael Shilman\nLiam DeBeasi\nJean Mertz\nIngo Bürk\nTobias Reich\n"
     ]
    }
   ],
   "source": [
    "nm = soup.find_all('h1', class_=\"h3 lh-condensed\")\n",
    "nombres = []\n",
    "for i in nm:\n",
    "    nombres.append(i.text)\n",
    "lista_nombres = []\n",
    "for n in nombres:\n",
    "    lista_nombres.append(n.replace(\"\\n\",\"\").strip())\n",
    "print(*lista_nombres, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagina2 = requests.get(url2)\n",
    "soup2 = BeautifulSoup(pagina2.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "l1ving / youtube-dl\nMobSF / Mobile-Security-Framework-MobSF\nmxrch / GHunt\ngto76 / python-cheatsheet\njofpin / trape\napache / incubator-superset\nswisskyrepo / PayloadsAllTheThings\nOWASP / CheatSheetSeries\nfacebookresearch / pifuhd\njerry-git / learn-python3\nethereum / eth2.0-deposit-cli\nfacebookresearch / wav2letter\nethereum / eth2.0-specs\nPaddlePaddle / PaddleX\nteja156 / microsoft-teams-class-attender\nssut / py-googletrans\npytorch / fairseq\nfacebookresearch / pytorch3d\nfloodsung / Deep-Learning-Papers-Reading-Roadmap\ndask / dask\ndeepset-ai / haystack\naws-samples / aws-cdk-examples\nopenatx / uiautomator2\nmoranzcw / Computer-Networking-A-Top-Down-Approach-NOTES\naiogram / aiogram\n"
     ]
    }
   ],
   "source": [
    "rep = soup2.find_all('h1', class_=\"h3 lh-condensed\")\n",
    "repositorios = []\n",
    "for i in rep:\n",
    "    repositorios.append(i.text)\n",
    "repo_limpio = []\n",
    "for r in repositorios:\n",
    "    repo_limpio.append(r.replace(\"\\n\",\"\").replace(\"/      \",\"/ \").strip())\n",
    "print(*repo_limpio,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = requests.get(url3)\n",
    "soup3 = BeautifulSoup(img.content, 'html.parser')\n",
    "import regex as re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg\n//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg\n//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg\n"
     ]
    }
   ],
   "source": [
    "images = soup3.find_all('img', {'src':re.compile('.jpg')}) \n",
    "print(*map(lambda x: x['src'],images), sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve all links to pages on Wikipedia that refer to some kind of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url4 ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnk = requests.get(url4)\n",
    "soup4 = BeautifulSoup(lnk.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/wiki/Pythons\n/wiki/Python_(genus)\n/wiki/Python_(programming_language)\n/wiki/Python_of_Aenus\n/wiki/Python_(painter)\n/wiki/Python_of_Byzantium\n/wiki/Python_of_Catana\n/wiki/Python_Anghelo\n/wiki/Python_(Efteling)\n/wiki/Python_(Busch_Gardens_Tampa_Bay)\n/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n/wiki/Python_(automobile_maker)\n/wiki/Python_(Ford_prototype)\n/wiki/Python_(missile)\n/wiki/Python_(nuclear_primary)\n/wiki/Colt_Python\n/wiki/Python_(film)\n/wiki/Python_(mythology)\n/wiki/Monty_Python\n/wiki/Python_(Monty)_Pictures\n/wiki/Python\n/wiki/Talk:Python\n/wiki/Python\n/wiki/Special:WhatLinksHere/Python\n/wiki/Special:RecentChangesLinked/Python\n"
     ]
    }
   ],
   "source": [
    "link = soup4.find_all(href = re.compile(\"Python\"))\n",
    "enlaces = []\n",
    "for i in link:\n",
    "    if i[\"href\"].startswith(\"/wiki/\"):\n",
    "        enlaces.append(i[\"href\"])\n",
    "    \n",
    "print(*enlaces, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url7 = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlesusa = requests.get(url7)\n",
    "soup7 = BeautifulSoup(titlesusa.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n          Title 1 - General Provisions ٭\n\n\n\n          Title 3 - The President ٭\n\n\n\n          Title 4 - Flag and Seal, Seat of Government, and the States ٭\n\n\n\n          Title 5 - Government Organization and Employees ٭\n\n\n\n          Title 9 - Arbitration ٭\n\n\n\n          Title 10 - Armed Forces ٭\n\n\n\n          Title 13 - Census ٭\n\n\n\n          Title 14 - Coast Guard ٭\n\n\n\n          Title 17 - Copyrights ٭\n\n\n\n          Title 23 - Highways ٭\n\n\n\n          Title 28 - Judiciary and Judicial Procedure ٭\n\n\n\n          Title 35 - Patents ٭\n\n\n\n          Title 36 - Patriotic and National Observances, Ceremonies, and Organizations ٭\n\n\n\n          Title 37 - Pay and Allowances of the Uniformed Services ٭\n\n\n\n          Title 39 - Postal Service ٭\n\n\n\n          Title 40 - Public Buildings, Property, and Works ٭\n\n\n\n          Title 41 - Public Contracts ٭\n\n\n\n          Title 44 - Public Printing and Documents ٭\n\n\n\n          Title 46 - Shipping ٭\n\n\n\n          Title 49 - Transportation ٭\n\n\n\n          Title 54 - National Park Service and Related Programs ٭\n\n"
     ]
    }
   ],
   "source": [
    "leyes = soup7.find_all(\"div\", class_=\"uscitem\")\n",
    "leyesusa = []\n",
    "for i in leyes:\n",
    "    if i.findChild(class_ = \"footnote\") and i.findChild(class_=\"usctitle\"):\n",
    "        leyesusa.append(i.findChild(class_=\"usctitle\").text)\n",
    "    \n",
    "print(*leyesusa, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url5 = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ARNOLDO JIMENEZ\nJASON DEREK BROWN\nALEXIS FLORES\nJOSE RODOLFO VILLARREAL-HERNANDEZ\nEUGENE PALMER\nRAFAEL CARO-QUINTERO\nROBERT WILLIAM FISHER\nBHADRESHKUMAR CHETANBHAI PATEL\nALEJANDRO ROSALES CASTILLO\nYASER ABDEL SAID\n"
     ]
    }
   ],
   "source": [
    "mwt = requests.get(url5)\n",
    "soup5 = BeautifulSoup(mwt.content, 'html.parser')\n",
    " \n",
    "\n",
    "mostwan = soup5.find_all('h3', class_= \"title\")\n",
    "most_wanted = []\n",
    "for i in mostwan:\n",
    "    most_wanted.append(i.text.strip(\"\\n\"))\n",
    "print(*most_wanted, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url8 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "lng = requests.get(url8)\n",
    "soup8 = BeautifulSoup(lng.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('English', '6183000+ articles')\n('Español', '1637000+ artículos')\n('日本語', '1235000+ 記事')\n('Deutsch', '2495000+ Artikel')\n('Русский', '1672000+ статей')\n('Français', '2262000+ articles')\n('Italiano', '1645000+ voci')\n('中文', '1155000+ 條目')\n('Português', '1045000+ artigos')\n('Polski', '1435000+ haseł')\n"
     ]
    }
   ],
   "source": [
    "lenguas = soup8.find_all(\"div\", class_=\"central-featured-lang\")\n",
    "lenguaswiki = []\n",
    "for i in lenguas:\n",
    "    if i.findChild(\"strong\") and i.findChild(\"small\"):\n",
    "        lenguaswiki.append((i.findChild(\"strong\").text, i.findChild(\"small\").text.replace(u\"\\xa0\", \"\")))\n",
    "    \n",
    "print(*lenguaswiki, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url6 = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Business and economy\nCrime and justice\nDefence\nEducation\nEnvironment\nGovernment\nGovernment spending\nHealth\nMapping\nSociety\nTowns and cities\nTransport\n"
     ]
    }
   ],
   "source": [
    "govdata = requests.get(url6)\n",
    "soup6 = BeautifulSoup(govdata.content, 'html.parser')\n",
    " \n",
    "\n",
    "govern_data = soup6.find_all('h3', class_= \"govuk-heading-s dgu-topics__heading\")\n",
    "kind_data = []\n",
    "for i in govern_data:\n",
    "    kind_data.append(i.text)\n",
    "print(*kind_data, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "urlpd = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "df_list = pd.read_html(urlpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Rank  Speakers(millions)                            Language\n",
       "0     1               918.0                    Mandarin Chinese\n",
       "1     2               480.0                             Spanish\n",
       "2     3               379.0                             English\n",
       "3     4               341.0  Hindi (Sanskritised Hindustani)[9]\n",
       "4     5               228.0                             Bengali\n",
       "5     6               221.0                          Portuguese\n",
       "6     7               154.0                             Russian\n",
       "7     8               128.0                            Japanese\n",
       "8     9                92.7                 Western Punjabi[10]\n",
       "9    10                83.1                             Marathi"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rank</th>\n      <th>Speakers(millions)</th>\n      <th>Language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>918.0</td>\n      <td>Mandarin Chinese</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>480.0</td>\n      <td>Spanish</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>379.0</td>\n      <td>English</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>341.0</td>\n      <td>Hindi (Sanskritised Hindustani)[9]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>228.0</td>\n      <td>Bengali</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>221.0</td>\n      <td>Portuguese</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>154.0</td>\n      <td>Russian</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>128.0</td>\n      <td>Japanese</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>92.7</td>\n      <td>Western Punjabi[10]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>83.1</td>\n      <td>Marathi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 248
    }
   ],
   "source": [
    "dataframe = df_list[0][:10]\n",
    "dataframe.iloc[:, [0,2,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Stepping up the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(city):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}